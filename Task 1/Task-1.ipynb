{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5041dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./BBC/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9d22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66d4f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entertainment': 386, 'business': 510, 'sport': 511, 'politics': 417, 'tech': 401}\n"
     ]
    }
   ],
   "source": [
    "SamplePerClass = {} \n",
    "for root, directories, files in os.walk(directory, topdown=False):\n",
    "    for name in directories:\n",
    "        counter = 0\n",
    "        for root2, directories2, files2 in os.walk(os.path.join(root, name), topdown=False):\n",
    "            for file in files2:\n",
    "                counter += 1\n",
    "        SamplePerClass[name] = counter\n",
    "print(SamplePerClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd190e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(SamplePerClass)), list(SamplePerClass.values()), align='center')\n",
    "plt.xticks(range(len(SamplePerClass)), list(SamplePerClass.keys()))\n",
    "plt.savefig(\"BBC-distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81287d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_files(directory, encoding=\"latin1\") #returns a scikit \"bunch\", a holder object (accessed like a dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1391417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c9348c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 29421)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bagOfWords = vectorizer.fit_transform(data.data) #gather all words and assign indices\n",
    "bagOfWords = vectorizer.transform(data.data) #assign word counts\n",
    "bagOfWords.shape\n",
    "#bag of words is a matrix where \n",
    "    #the colomns are the vocabulary words\n",
    "    #the rows are the number of total documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2769f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1780, 29421)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors, X_test_vectors, y_train_vectors, y_test_vectors = train_test_split(bagOfWords, data.target, train_size=0.80, test_size=0.20, random_state=None)\n",
    "X_train_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0eea0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_Cls = MultinomialNB()\n",
    "NB_Cls.fit(X_train_vectors, y_train_vectors)\n",
    "predicted = NB_Cls.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a8d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"bbc-performance.txt\",\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d2ffd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"(a)\\n\")\n",
    "output.write(\"*****************************************\\n\")\n",
    "output.write(\"Multinomial Naive Bayes classifier, try 1\\n\")\n",
    "output.write(\"*****************************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c717b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92   1   1   0   2]\n",
      " [  0  68   2   0   1]\n",
      " [  0   1  80   0   0]\n",
      " [  0   0   0 106   0]\n",
      " [  2   0   0   0  89]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(b)\\n\")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test_vectors, predicted)\n",
    "print(cm)\n",
    "output.write(str(cm) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96be5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.98      0.96      0.97        96\n",
      "entertainment       0.97      0.96      0.96        71\n",
      "     politics       0.96      0.99      0.98        81\n",
      "        sport       1.00      1.00      1.00       106\n",
      "         tech       0.97      0.98      0.97        91\n",
      "\n",
      "     accuracy                           0.98       445\n",
      "    macro avg       0.98      0.98      0.98       445\n",
      " weighted avg       0.98      0.98      0.98       445\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(c)\\n\")\n",
    "\n",
    "cr = classification_report(y_test_vectors, predicted, target_names=data.target_names)\n",
    "print(cr)\n",
    "output.write(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69d149f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.9775280898876404\n",
      "Macro F1 Score = 0.9762494822899507\n",
      "Weighted F1 Score = 0.9775027910163279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(d)\\n\")\n",
    "\n",
    "asc = \"Accuracy Score = \" + str(accuracy_score(y_test_vectors, predicted))\n",
    "f1_macro = \"Macro F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='macro'))\n",
    "f1_weighted = \"Weighted F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='weighted'))\n",
    "\n",
    "print(asc)\n",
    "print(f1_macro)\n",
    "print(f1_weighted)\n",
    "\n",
    "output.write(asc)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_macro)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_weighted)\n",
    "output.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c17aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability for each class:\n",
      "\n",
      "business\t = -1.4585026694608167\n",
      "\n",
      "entertainment\t = -1.7317960044604979\n",
      "\n",
      "politics\t = -1.6672574833229268\n",
      "\n",
      "sport\t\t = -1.4804815761795922\n",
      "\n",
      "tech\t\t = -1.7477963458069388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.write(\"\\n(e)\\n\")\n",
    "\n",
    "print(\"Prior Probability for each class:\\n\")\n",
    "output.write(\"Prior Probability for each class:\\n\")\n",
    "priorsDic = dict(zip(data.target_names, NB_Cls.class_log_prior_))\n",
    "\n",
    "avr =0\n",
    "sums =0\n",
    "for key in priorsDic:\n",
    "    sums += len(key)\n",
    "avr = sums / len(priorsDic)\n",
    "\n",
    "for key in priorsDic:\n",
    "    if (len(key) < int(avr)):\n",
    "        s = str(key) + '\\t\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    else:\n",
    "        s = str(key) + '\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    print(s)\n",
    "    output.write(str(s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42024ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 29421 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(f)\\n\")\n",
    "\n",
    "vocabulary = len(vectorizer.vocabulary_)\n",
    "    \n",
    "print(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "output.write(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc56a80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment: 124893\n",
      "\n",
      "business: 164663\n",
      "\n",
      "sport: 162953\n",
      "\n",
      "politics: 185208\n",
      "\n",
      "tech: 198640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.write(\"\\n(g)\\n\")\n",
    "\n",
    "ClassDict = {} \n",
    "for root, directories, files in os.walk(directory, topdown=False):\n",
    "    for name in directories:\n",
    "        value = []\n",
    "        for root2, directories2, files2 in os.walk(os.path.join(root, name), topdown=False):\n",
    "            for file in files2:\n",
    "                f = open(os.path.join(root2, file), \"r\", encoding=\"latin1\")\n",
    "                value.append(f.read())\n",
    "        ClassDict[name] = value\n",
    "\n",
    "classDicTotal = {}\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) #builds a matrix of feature/indices and their occurence\n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() #outputs a dataframe obj of counts of all words\n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    OutputStr += \": \" + str(Classtotal) + \"\\n\"\n",
    "    classDicTotal[className] = Classtotal\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "109117cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-tokens in the corpus: 836357 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(h)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "    \n",
    "print(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "output.write(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e23daaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment: 17746 words with frequence of zero or, 2.12%\n",
      "\n",
      "business: 17538 words with frequence of zero or, 2.10%\n",
      "\n",
      "sport: 18850 words with frequence of zero or, 2.25%\n",
      "\n",
      "politics: 18201 words with frequence of zero or, 2.18%\n",
      "\n",
      "tech: 17323 words with frequence of zero or, 2.07%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.write(\"\\n(i)\\n\")\n",
    "   \n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    dfDiff = dfSum.sub(dfclassBOWSum)\n",
    "    \n",
    "    counter = 0\n",
    "    for i in dfDiff.index:\n",
    "        if math.isnan(dfDiff[i]):\n",
    "            counter += 1\n",
    "    OutputStr += \": \" + str(counter) + \" words with frequence of zero or, {0:.2f}%\".format(counter/totalSum * 100) + \"\\n\"\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15619c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b0961ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d3c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entertainment => the -2.7149437879850633\n",
      "entertainment => to -3.7617126856825998\n",
      "\n",
      "business => the -2.722597113359981\n",
      "business => to -3.477409294065489\n",
      "\n",
      "sport => the -2.8257787746847027\n",
      "sport => to -3.541441174105334\n",
      "\n",
      "politics => the -2.7213559022305507\n",
      "politics => to -3.429053604052196\n",
      "\n",
      "tech => the -2.8313071877023757\n",
      "tech => to -3.435352413042588\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.write(\"\\n(k)\\n\")\n",
    "   \n",
    "popWords = dfSum.nlargest(2)\n",
    "outputStr = \"\"\n",
    "\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    \n",
    "    \n",
    "    for i in popWords.index:\n",
    "        outputStr += str(className) + \" => \" + str(i) + \" \" + str(math.log(dfclassBOWSum[i]/Classtotal)) + \"\\n\"\n",
    "    outputStr += \"\\n\"\n",
    "\n",
    "print(outputStr)\n",
    "output.write(outputStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563b8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92   1   1   0   2]\n",
      " [  0  68   2   0   1]\n",
      " [  0   1  80   0   0]\n",
      " [  0   0   0 106   0]\n",
      " [  2   0   0   0  89]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.98      0.96      0.97        96\n",
      "entertainment       0.97      0.96      0.96        71\n",
      "     politics       0.96      0.99      0.98        81\n",
      "        sport       1.00      1.00      1.00       106\n",
      "         tech       0.97      0.98      0.97        91\n",
      "\n",
      "     accuracy                           0.98       445\n",
      "    macro avg       0.98      0.98      0.98       445\n",
      " weighted avg       0.98      0.98      0.98       445\n",
      "\n",
      "Accuracy Score = 0.9775280898876404\n",
      "Macro F1 Score = 0.9762494822899507\n",
      "Weighted F1 Score = 0.9775027910163279\n",
      "Prior Probability for each class:\n",
      "\n",
      "business\t = -1.4585026694608167\n",
      "\n",
      "entertainment\t = -1.7317960044604979\n",
      "\n",
      "politics\t = -1.6672574833229268\n",
      "\n",
      "sport\t\t = -1.4804815761795922\n",
      "\n",
      "tech\t\t = -1.7477963458069388\n",
      "\n",
      "Size of vocabulary: 29421 words\n",
      "entertainment: 124893\n",
      "\n",
      "business: 164663\n",
      "\n",
      "sport: 162953\n",
      "\n",
      "politics: 185208\n",
      "\n",
      "tech: 198640\n",
      "\n",
      "Word-tokens in the corpus: 836357 words\n",
      "entertainment: 17746 words with frequence of zero or, 2.12%\n",
      "\n",
      "business: 17538 words with frequence of zero or, 2.10%\n",
      "\n",
      "sport: 18850 words with frequence of zero or, 2.25%\n",
      "\n",
      "politics: 18201 words with frequence of zero or, 2.18%\n",
      "\n",
      "tech: 17323 words with frequence of zero or, 2.07%\n",
      "\n",
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n",
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n",
      "entertainment => the -2.7149437879850633\n",
      "entertainment => to -3.7617126856825998\n",
      "\n",
      "business => the -2.722597113359981\n",
      "business => to -3.477409294065489\n",
      "\n",
      "sport => the -2.8257787746847027\n",
      "sport => to -3.541441174105334\n",
      "\n",
      "politics => the -2.7213559022305507\n",
      "politics => to -3.429053604052196\n",
      "\n",
      "tech => the -2.8313071877023757\n",
      "tech => to -3.435352413042588\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_Cls = MultinomialNB()\n",
    "NB_Cls.fit(X_train_vectors, y_train_vectors)\n",
    "predicted = NB_Cls.predict(X_test_vectors)\n",
    "\n",
    "output.write(\"(a)\\n\")\n",
    "output.write(\"*****************************************\\n\")\n",
    "output.write(\"Multinomial Naive Bayes classifier, try 2\\n\")\n",
    "output.write(\"*****************************************\\n\")\n",
    "\n",
    "output.write(\"\\n(b)\\n\")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test_vectors, predicted)\n",
    "print(cm)\n",
    "output.write(str(cm) + \"\\n\")\n",
    "\n",
    "output.write(\"\\n(c)\\n\")\n",
    "\n",
    "cr = classification_report(y_test_vectors, predicted, target_names=data.target_names)\n",
    "print(cr)\n",
    "output.write(cr)\n",
    "\n",
    "output.write(\"\\n(d)\\n\")\n",
    "\n",
    "asc = \"Accuracy Score = \" + str(accuracy_score(y_test_vectors, predicted))\n",
    "f1_macro = \"Macro F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='macro'))\n",
    "f1_weighted = \"Weighted F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='weighted'))\n",
    "\n",
    "print(asc)\n",
    "print(f1_macro)\n",
    "print(f1_weighted)\n",
    "\n",
    "output.write(asc)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_macro)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_weighted)\n",
    "output.write(\"\\n\")\n",
    "\n",
    "output.write(\"\\n(e)\\n\")\n",
    "\n",
    "print(\"Prior Probability for each class:\\n\")\n",
    "output.write(\"Prior Probability for each class:\\n\")\n",
    "priorsDic = dict(zip(data.target_names, NB_Cls.class_log_prior_))\n",
    "\n",
    "avr =0\n",
    "sums =0\n",
    "for key in priorsDic:\n",
    "    sums += len(key)\n",
    "avr = sums / len(priorsDic)\n",
    "\n",
    "for key in priorsDic:\n",
    "    if (len(key) < int(avr)):\n",
    "        s = str(key) + '\\t\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    else:\n",
    "        s = str(key) + '\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    print(s)\n",
    "    output.write(str(s))\n",
    "\n",
    "output.write(\"\\n(f)\\n\")\n",
    "\n",
    "vocabulary = len(vectorizer.vocabulary_)\n",
    "    \n",
    "print(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "output.write(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "\n",
    "\n",
    "output.write(\"\\n(g)\\n\")\n",
    "\n",
    "ClassDict = {} \n",
    "for root, directories, files in os.walk(directory, topdown=False):\n",
    "    for name in directories:\n",
    "        value = []\n",
    "        for root2, directories2, files2 in os.walk(os.path.join(root, name), topdown=False):\n",
    "            for file in files2:\n",
    "                f = open(os.path.join(root2, file), \"r\", encoding=\"latin1\")\n",
    "                value.append(f.read())\n",
    "        ClassDict[name] = value\n",
    "\n",
    "classDicTotal = {}\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) #builds a matrix of feature/indices and their occurence\n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() #outputs a dataframe obj of counts of all words\n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    OutputStr += \": \" + str(Classtotal) + \"\\n\"\n",
    "    classDicTotal[className] = Classtotal\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    \n",
    "output.write(\"\\n(h)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "    \n",
    "print(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "output.write(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "\n",
    "output.write(\"\\n(i)\\n\")\n",
    "   \n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    dfDiff = dfSum.sub(dfclassBOWSum)\n",
    "    \n",
    "    counter = 0\n",
    "    for i in dfDiff.index:\n",
    "        if math.isnan(dfDiff[i]):\n",
    "            counter += 1\n",
    "    OutputStr += \": \" + str(counter) + \" words with frequence of zero or, {0:.2f}%\".format(counter/totalSum * 100) + \"\\n\"\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    \n",
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "\n",
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "\n",
    "output.write(\"\\n(k)\\n\")\n",
    "   \n",
    "popWords = dfSum.nlargest(2)\n",
    "outputStr = \"\"\n",
    "\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    \n",
    "    \n",
    "    for i in popWords.index:\n",
    "        outputStr += str(className) + \" => \" + str(i) + \" \" + str(math.log(dfclassBOWSum[i]/Classtotal)) + \"\\n\"\n",
    "    outputStr += \"\\n\"\n",
    "\n",
    "print(outputStr)\n",
    "output.write(outputStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b4625a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92   1   0   0   3]\n",
      " [  0  68   2   0   1]\n",
      " [  3   0  78   0   0]\n",
      " [  0   0   0 106   0]\n",
      " [  3   1   0   0  87]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.96      0.95        96\n",
      "entertainment       0.97      0.96      0.96        71\n",
      "     politics       0.97      0.96      0.97        81\n",
      "        sport       1.00      1.00      1.00       106\n",
      "         tech       0.96      0.96      0.96        91\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Accuracy Score = 0.9685393258426966\n",
      "Macro F1 Score = 0.9675961341524918\n",
      "Weighted F1 Score = 0.9685804223482879\n",
      "Prior Probability for each class:\n",
      "\n",
      "business\t = -1.4585026694608167\n",
      "\n",
      "entertainment\t = -1.7317960044604979\n",
      "\n",
      "politics\t = -1.6672574833229268\n",
      "\n",
      "sport\t\t = -1.4804815761795922\n",
      "\n",
      "tech\t\t = -1.7477963458069388\n",
      "\n",
      "Size of vocabulary: 29421 words\n",
      "entertainment: 124893\n",
      "\n",
      "business: 164663\n",
      "\n",
      "sport: 162953\n",
      "\n",
      "politics: 185208\n",
      "\n",
      "tech: 198640\n",
      "\n",
      "Word-tokens in the corpus: 836357 words\n",
      "entertainment: 17746 words with frequence of zero or, 2.12%\n",
      "\n",
      "business: 17538 words with frequence of zero or, 2.10%\n",
      "\n",
      "sport: 18850 words with frequence of zero or, 2.25%\n",
      "\n",
      "politics: 18201 words with frequence of zero or, 2.18%\n",
      "\n",
      "tech: 17323 words with frequence of zero or, 2.07%\n",
      "\n",
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n",
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n",
      "entertainment => the -2.7149437879850633\n",
      "entertainment => to -3.7617126856825998\n",
      "\n",
      "business => the -2.722597113359981\n",
      "business => to -3.477409294065489\n",
      "\n",
      "sport => the -2.8257787746847027\n",
      "sport => to -3.541441174105334\n",
      "\n",
      "politics => the -2.7213559022305507\n",
      "politics => to -3.429053604052196\n",
      "\n",
      "tech => the -2.8313071877023757\n",
      "tech => to -3.435352413042588\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_Cls = MultinomialNB(alpha=0.0001)\n",
    "NB_Cls.fit(X_train_vectors, y_train_vectors)\n",
    "predicted = NB_Cls.predict(X_test_vectors)\n",
    "\n",
    "output.write(\"(a)\\n\")\n",
    "output.write(\"**************************************************************\\n\")\n",
    "output.write(\"Multinomial Naive Bayes classifier, try 3 (smoothing 0.0001\\n\")\n",
    "output.write(\"**************************************************************\\n\")\n",
    "\n",
    "output.write(\"\\n(b)\\n\")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test_vectors, predicted)\n",
    "print(cm)\n",
    "output.write(str(cm) + \"\\n\")\n",
    "\n",
    "output.write(\"\\n(c)\\n\")\n",
    "\n",
    "cr = classification_report(y_test_vectors, predicted, target_names=data.target_names)\n",
    "print(cr)\n",
    "output.write(cr)\n",
    "\n",
    "output.write(\"\\n(d)\\n\")\n",
    "\n",
    "asc = \"Accuracy Score = \" + str(accuracy_score(y_test_vectors, predicted))\n",
    "f1_macro = \"Macro F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='macro'))\n",
    "f1_weighted = \"Weighted F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='weighted'))\n",
    "\n",
    "print(asc)\n",
    "print(f1_macro)\n",
    "print(f1_weighted)\n",
    "\n",
    "output.write(asc)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_macro)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_weighted)\n",
    "output.write(\"\\n\")\n",
    "\n",
    "output.write(\"\\n(e)\\n\")\n",
    "\n",
    "print(\"Prior Probability for each class:\\n\")\n",
    "output.write(\"Prior Probability for each class:\\n\")\n",
    "priorsDic = dict(zip(data.target_names, NB_Cls.class_log_prior_))\n",
    "\n",
    "avr =0\n",
    "sums =0\n",
    "for key in priorsDic:\n",
    "    sums += len(key)\n",
    "avr = sums / len(priorsDic)\n",
    "\n",
    "for key in priorsDic:\n",
    "    if (len(key) < int(avr)):\n",
    "        s = str(key) + '\\t\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    else:\n",
    "        s = str(key) + '\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    print(s)\n",
    "    output.write(str(s))\n",
    "\n",
    "output.write(\"\\n(f)\\n\")\n",
    "\n",
    "vocabulary = len(vectorizer.vocabulary_)\n",
    "    \n",
    "print(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "output.write(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "\n",
    "\n",
    "output.write(\"\\n(g)\\n\")\n",
    "\n",
    "ClassDict = {} \n",
    "for root, directories, files in os.walk(directory, topdown=False):\n",
    "    for name in directories:\n",
    "        value = []\n",
    "        for root2, directories2, files2 in os.walk(os.path.join(root, name), topdown=False):\n",
    "            for file in files2:\n",
    "                f = open(os.path.join(root2, file), \"r\", encoding=\"latin1\")\n",
    "                value.append(f.read())\n",
    "        ClassDict[name] = value\n",
    "\n",
    "classDicTotal = {}\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) #builds a matrix of feature/indices and their occurence\n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() #outputs a dataframe obj of counts of all words\n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    OutputStr += \": \" + str(Classtotal) + \"\\n\"\n",
    "    classDicTotal[className] = Classtotal\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    \n",
    "output.write(\"\\n(h)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "    \n",
    "print(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "output.write(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "\n",
    "output.write(\"\\n(i)\\n\")\n",
    "   \n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    dfDiff = dfSum.sub(dfclassBOWSum)\n",
    "    \n",
    "    counter = 0\n",
    "    for i in dfDiff.index:\n",
    "        if math.isnan(dfDiff[i]):\n",
    "            counter += 1\n",
    "    OutputStr += \": \" + str(counter) + \" words with frequence of zero or, {0:.2f}%\".format(counter/totalSum * 100) + \"\\n\"\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    \n",
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "\n",
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "\n",
    "output.write(\"\\n(k)\\n\")\n",
    "   \n",
    "popWords = dfSum.nlargest(2)\n",
    "outputStr = \"\"\n",
    "\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    \n",
    "    \n",
    "    for i in popWords.index:\n",
    "        outputStr += str(className) + \" => \" + str(i) + \" \" + str(math.log(dfclassBOWSum[i]/Classtotal)) + \"\\n\"\n",
    "    outputStr += \"\\n\"\n",
    "\n",
    "print(outputStr)\n",
    "output.write(outputStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a49dcaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 92   1   1   0   2]\n",
      " [  0  68   2   0   1]\n",
      " [  0   1  80   0   0]\n",
      " [  0   0   0 106   0]\n",
      " [  2   0   0   0  89]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.98      0.96      0.97        96\n",
      "entertainment       0.97      0.96      0.96        71\n",
      "     politics       0.96      0.99      0.98        81\n",
      "        sport       1.00      1.00      1.00       106\n",
      "         tech       0.97      0.98      0.97        91\n",
      "\n",
      "     accuracy                           0.98       445\n",
      "    macro avg       0.98      0.98      0.98       445\n",
      " weighted avg       0.98      0.98      0.98       445\n",
      "\n",
      "Accuracy Score = 0.9775280898876404\n",
      "Macro F1 Score = 0.9762494822899507\n",
      "Weighted F1 Score = 0.9775027910163279\n",
      "Prior Probability for each class:\n",
      "\n",
      "business\t = -1.4585026694608167\n",
      "\n",
      "entertainment\t = -1.7317960044604979\n",
      "\n",
      "politics\t = -1.6672574833229268\n",
      "\n",
      "sport\t\t = -1.4804815761795922\n",
      "\n",
      "tech\t\t = -1.7477963458069388\n",
      "\n",
      "Size of vocabulary: 29421 words\n",
      "entertainment: 124893\n",
      "\n",
      "business: 164663\n",
      "\n",
      "sport: 162953\n",
      "\n",
      "politics: 185208\n",
      "\n",
      "tech: 198640\n",
      "\n",
      "Word-tokens in the corpus: 836357 words\n",
      "entertainment: 17746 words with frequence of zero or, 2.12%\n",
      "\n",
      "business: 17538 words with frequence of zero or, 2.10%\n",
      "\n",
      "sport: 18850 words with frequence of zero or, 2.25%\n",
      "\n",
      "politics: 18201 words with frequence of zero or, 2.18%\n",
      "\n",
      "tech: 17323 words with frequence of zero or, 2.07%\n",
      "\n",
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n",
      "There are 10005 words with a frequency of 1 in the corpus or 1.20%\n",
      "entertainment => the -2.7149437879850633\n",
      "entertainment => to -3.7617126856825998\n",
      "\n",
      "business => the -2.722597113359981\n",
      "business => to -3.477409294065489\n",
      "\n",
      "sport => the -2.8257787746847027\n",
      "sport => to -3.541441174105334\n",
      "\n",
      "politics => the -2.7213559022305507\n",
      "politics => to -3.429053604052196\n",
      "\n",
      "tech => the -2.8313071877023757\n",
      "tech => to -3.435352413042588\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "351"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_Cls = MultinomialNB(alpha=0.9)\n",
    "NB_Cls.fit(X_train_vectors, y_train_vectors)\n",
    "predicted = NB_Cls.predict(X_test_vectors)\n",
    "\n",
    "output.write(\"(a)\\n\")\n",
    "output.write(\"**************************************************************\\n\")\n",
    "output.write(\"Multinomial Naive Bayes classifier, try 4 (smoothing 0.9\\n\")\n",
    "output.write(\"**************************************************************\\n\")\n",
    "\n",
    "output.write(\"\\n(b)\\n\")\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test_vectors, predicted)\n",
    "print(cm)\n",
    "output.write(str(cm) + \"\\n\")\n",
    "\n",
    "output.write(\"\\n(c)\\n\")\n",
    "\n",
    "cr = classification_report(y_test_vectors, predicted, target_names=data.target_names)\n",
    "print(cr)\n",
    "output.write(cr)\n",
    "\n",
    "output.write(\"\\n(d)\\n\")\n",
    "\n",
    "asc = \"Accuracy Score = \" + str(accuracy_score(y_test_vectors, predicted))\n",
    "f1_macro = \"Macro F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='macro'))\n",
    "f1_weighted = \"Weighted F1 Score = \" +str(f1_score(y_test_vectors, predicted, average='weighted'))\n",
    "\n",
    "print(asc)\n",
    "print(f1_macro)\n",
    "print(f1_weighted)\n",
    "\n",
    "output.write(asc)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_macro)\n",
    "output.write(\"\\n\")\n",
    "output.write(f1_weighted)\n",
    "output.write(\"\\n\")\n",
    "\n",
    "output.write(\"\\n(e)\\n\")\n",
    "\n",
    "print(\"Prior Probability for each class:\\n\")\n",
    "output.write(\"Prior Probability for each class:\\n\")\n",
    "priorsDic = dict(zip(data.target_names, NB_Cls.class_log_prior_))\n",
    "\n",
    "avr =0\n",
    "sums =0\n",
    "for key in priorsDic:\n",
    "    sums += len(key)\n",
    "avr = sums / len(priorsDic)\n",
    "\n",
    "for key in priorsDic:\n",
    "    if (len(key) < int(avr)):\n",
    "        s = str(key) + '\\t\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    else:\n",
    "        s = str(key) + '\\t = ' + str(priorsDic[key]) + '\\n'\n",
    "    print(s)\n",
    "    output.write(str(s))\n",
    "\n",
    "output.write(\"\\n(f)\\n\")\n",
    "\n",
    "vocabulary = len(vectorizer.vocabulary_)\n",
    "    \n",
    "print(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "output.write(\"Size of vocabulary: \" + str(vocabulary) + \" words\")\n",
    "\n",
    "\n",
    "output.write(\"\\n(g)\\n\")\n",
    "\n",
    "ClassDict = {} \n",
    "for root, directories, files in os.walk(directory, topdown=False):\n",
    "    for name in directories:\n",
    "        value = []\n",
    "        for root2, directories2, files2 in os.walk(os.path.join(root, name), topdown=False):\n",
    "            for file in files2:\n",
    "                f = open(os.path.join(root2, file), \"r\", encoding=\"latin1\")\n",
    "                value.append(f.read())\n",
    "        ClassDict[name] = value\n",
    "\n",
    "classDicTotal = {}\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) #builds a matrix of feature/indices and their occurence\n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() #outputs a dataframe obj of counts of all words\n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    OutputStr += \": \" + str(Classtotal) + \"\\n\"\n",
    "    classDicTotal[className] = Classtotal\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    \n",
    "output.write(\"\\n(h)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "    \n",
    "print(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "output.write(\"Word-tokens in the corpus: \" + str(totalSum) + \" words\")\n",
    "\n",
    "output.write(\"\\n(i)\\n\")\n",
    "   \n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    dfDiff = dfSum.sub(dfclassBOWSum)\n",
    "    \n",
    "    counter = 0\n",
    "    for i in dfDiff.index:\n",
    "        if math.isnan(dfDiff[i]):\n",
    "            counter += 1\n",
    "    OutputStr += \": \" + str(counter) + \" words with frequence of zero or, {0:.2f}%\".format(counter/totalSum * 100) + \"\\n\"\n",
    "    \n",
    "    print(OutputStr)\n",
    "    output.write(OutputStr)\n",
    "    \n",
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "\n",
    "output.write(\"\\n(j)\\n\")\n",
    "   \n",
    "dfBagOfWords = pd.DataFrame(bagOfWords.todense(), columns=vectorizer.get_feature_names())\n",
    "dfSum = dfBagOfWords.sum() #outputs a dataframe obj of counts of all words\n",
    "totalSum = dfSum.sum()\n",
    "\n",
    "counter = 0;\n",
    "for i in dfSum.index:\n",
    "    if dfSum[i] == 1:\n",
    "        counter += 1\n",
    "    \n",
    "print(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "output.write(\"There are \" + str(counter) + \" words with a frequency of 1 in the corpus or \" +  \n",
    "      \"{0:.2f}%\".format(counter/totalSum * 100))\n",
    "\n",
    "output.write(\"\\n(k)\\n\")\n",
    "   \n",
    "popWords = dfSum.nlargest(2)\n",
    "outputStr = \"\"\n",
    "\n",
    "for className in ClassDict:\n",
    "    OutputStr = \"\"\n",
    "    OutputStr += className\n",
    "    classVectorizer = CountVectorizer()\n",
    "    classBOW = classVectorizer.fit_transform(ClassDict[className]) \n",
    "    \n",
    "    dfclassBOW = pd.DataFrame(classBOW.todense(), columns=classVectorizer.get_feature_names())\n",
    "    dfclassBOWSum = dfclassBOW.sum() \n",
    "    Classtotal = dfclassBOWSum.sum()\n",
    "    \n",
    "    \n",
    "    for i in popWords.index:\n",
    "        outputStr += str(className) + \" => \" + str(i) + \" \" + str(math.log(dfclassBOWSum[i]/Classtotal)) + \"\\n\"\n",
    "    outputStr += \"\\n\"\n",
    "\n",
    "print(outputStr)\n",
    "output.write(outputStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "068d56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd6502b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775280898876404\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97        96\n",
      "           1       0.97      0.96      0.96        71\n",
      "           2       0.96      0.99      0.98        81\n",
      "           3       1.00      1.00      1.00       106\n",
      "           4       0.97      0.98      0.97        91\n",
      "\n",
      "    accuracy                           0.98       445\n",
      "   macro avg       0.98      0.98      0.98       445\n",
      "weighted avg       0.98      0.98      0.98       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#(b)\n",
    "\n",
    "print(accuracy_score(y_test_vectors, predicted))\n",
    "print(classification_report(y_test_vectors, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9d661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
