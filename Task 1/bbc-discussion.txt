{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf400
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 11.\
\
A) \
Since we have a different number of samples in each class, the best suited metric for this multiclass classification is the weighted-average F-measure as it takes into account the number of samples from each class. \
\
B)\
The reason why the performance is the same for step 8  is because there is no randomness attributed to the MultinomialNB. Thus, the performance is based on the input data alone. \
\
For step 9, the performance of the F1 score was 97%, lowered by 1%. This means that a smoothing of 0.0001 doesn\'92t help our model. \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 For step 10, the performance of the F1 score was 98%. Thus, a smoothing value of 0.9 is better for our model. }